{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MLF Notes","text":""},{"location":"#important-links","title":"Important Links","text":"<p>Bug</p> <p>If you find a formatting issue in displaying math equations, kindly refresh the page. This is a bug and I am working on it. Hope to remove it soon.</p>"},{"location":"02%20Supervised%20Learning%20-%20MLF/","title":"Supervised Learning","text":""},{"location":"02%20Supervised%20Learning%20-%20MLF/#basic-setup","title":"Basic setup","text":"<p>Here are some notations that will be used throughout the course.</p> \\[ \\mathbb{R} \\rightarrow \\text{Real numbers}\\\\ \\mathbb{R}_+ \\rightarrow \\text{Positive Real numbers}\\\\ \\mathbb{R}^d \\rightarrow d\\text{ dimensional vector of Real numbers}\\\\ x \\rightarrow \\text{vector, } x_j \\to j^{th} \\text{coordinate of } x\\\\x^1, x^2, x^3 ...x^n \\to \\text{Collection of }n \\text{ vectors}\\\\ x^i_j \\to j^{th} \\text{ coordinate of } i^{th} \\text{ vector}\\\\ (x_1)^2: \\text{Square of first coordinate of vector }x \\]"},{"location":"02%20Supervised%20Learning%20-%20MLF/#regression","title":"Regression","text":"<p>Regression at its core is curve fitting.</p> <p>Given some inputs and outputs in the form \\({(x^1, y^1), (x^2, y^2)...(x^n, y^n)}\\), we find a model \\(f(x^i)\\text{ is closest to } y^i\\)</p> <p>Also note that here,</p> \\[ x^i \\in \\mathbb{R}^d, y^i \\in \\mathbb{R} \\] <p>Remember</p> <p>The algorithm outputs a model \\(f:\\mathbb{R}^d\\to\\mathbb{R}\\)</p> <p>Loss</p> <p>Loss of the model \\(f\\) is $ \\text{Loss}[f] = \\frac{1}{n}\\sum_{i=1}^{n}(f(x^i)- y^i)^2$</p>"},{"location":"02%20Supervised%20Learning%20-%20MLF/#general-form-of-model","title":"General form of Model","text":"\\[ f(x) = w^Tx+b=\\sum_{j=1}^{d}w_jx_j + b \\]"},{"location":"02%20Supervised%20Learning%20-%20MLF/#classification","title":"Classification","text":"<p>Given some inputs and outputs in the form \\({(x^1, y^1), (x^2, y^2)...(x^n, y^n)}\\)</p> <p>Also note that here,</p> \\[ x^i \\in \\mathbb{R}^d, y^i \\in \\{-1, +1\\} \\] <p>Remember</p> <p>The algorithm outputs a model \\(f:\\mathbb{R}^d\\to\\{-1, +1\\}\\)</p> <p>Loss</p> <p>Loss of the model \\(f\\) is $ \\text{Loss}[f] = \\frac{1}{n}\\sum_{i=1}^{n}1(f(x^i) \\neq y^i)$</p>"},{"location":"02%20Supervised%20Learning%20-%20MLF/#general-form-of-model_1","title":"General form of Model","text":"\\[ f(x) = sign(w^Tx+b) \\]"},{"location":"03%20Unsupervised%20Learning/","title":"Unsupervised Learning","text":"<p>Unsupervised Learning is used to 'understand data'.</p> <p>In contrast to Supervised Learning where the data is some inputs and their outputs, the data in Unsupervised Learning is just a set of inputs and no outputs, i.e., data is of the form \\(\\{x^1,x^2,x^3,...,x^n\\}\\), where \\(x^i\\in\\mathbb{R}^d\\).</p> <p>Note</p> <p>They build models that compress, explain and group data.</p>"},{"location":"03%20Unsupervised%20Learning/#dimensionality-reduction","title":"Dimensionality Reduction","text":""},{"location":"03%20Unsupervised%20Learning/#basic-setup","title":"Basic Setup","text":"<p>Data: \\(\\{x^1,x^2,x^3...,x^n\\}\\), where \\(x^i\\in\\mathbb{R}^d\\)</p>"},{"location":"03%20Unsupervised%20Learning/#encoder","title":"Encoder","text":"<p>The goal of the encoder is to take in a \\(d\\)-dimensional vector and compress it to give a \\(d'\\)-dimensional vector.</p> <p>Mathematically, Encoder \\(f:\\mathbb{R}^d\\to\\mathbb{R}^{d'}\\)</p> <p>Tip</p> <p>Typically \\(d'&lt;d\\)</p>"},{"location":"03%20Unsupervised%20Learning/#decoder","title":"Decoder","text":"<p>The goal of the decoder is to take in a \\(d'\\)-dimensional vector and decompress it to give a \\(d\\)-dimensional vector.</p> <p>Mathematically, Decoder \\(g:\\mathbb{R}^{d'}\\to\\mathbb{R}^{d}\\)</p> <p>Tip</p> <p>Typically \\(d'&lt;d\\)</p>"},{"location":"03%20Unsupervised%20Learning/#goal","title":"Goal","text":"<p>Goal of the dimensionality reduction is \\(g(f(x^i))\\approx x^i\\)</p>"},{"location":"03%20Unsupervised%20Learning/#loss","title":"Loss","text":"<p>Loss of the dimensionality reduction is</p> \\[ \\text{Loss} = \\cfrac{1}{n}\\sum_{i=1}^{n}||g(f(x^i)) - x^i||^2 \\]"},{"location":"03%20Unsupervised%20Learning/#uses-of-dimensionality-reduction","title":"Uses of Dimensionality Reduction","text":"<p>Dimensionality Reduction finds its main uses in compression and reduction.</p> <p>Tip</p> <p>The Dimensionality Reduction comes up with two models, i.e. the Encoder and the Decoder.</p>"},{"location":"03%20Unsupervised%20Learning/#density-estimation","title":"Density Estimation","text":"<p>It gives out a probabilistic model, i.e., the output is a model that stores different configurations of reality.</p>"},{"location":"03%20Unsupervised%20Learning/#basic-setup_1","title":"Basic Setup","text":"<p>Data: \\(\\{x^1,x^2,x^3,\\dots,x^n\\}\\), where \\(x^i\\in\\mathbb{R}^d\\)</p>"},{"location":"03%20Unsupervised%20Learning/#probabilistic-model","title":"Probabilistic Model","text":"<p>It gives out a probabilistic model \\(P:\\mathbb{R}^d\\to\\mathbb{R}_+\\).</p> <p>Tip</p> <p>All outputs sums up to \\(1\\), i.e., \\(\\sum_{i=1}^nP(x^i) = 1\\)</p>"},{"location":"03%20Unsupervised%20Learning/#goal_1","title":"Goal","text":"<p>The goal of probability estimation is to to give a probabilistic model \\(P\\) such that \\(P(x)\\) is large if \\(x\\in\\text{Data}\\), and low otherwise.</p>"},{"location":"03%20Unsupervised%20Learning/#loss_1","title":"Loss","text":"<p>Loss for the probabilistic model \\(P\\) is negative log likelihood, i.e., </p> \\[ \\text{Loss} = \\cfrac{1}{n} \\sum_{i=1}^n-log(P(x^i)) \\]"},{"location":"TODO/","title":"TODO","text":"<p>ADD</p>"},{"location":"TODO/#week-2","title":"Week-2","text":"<ul> <li>Lecture 6</li> </ul>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/","title":"Sets and Functions","text":""},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#some-important-symbols","title":"Some important symbols","text":"Symbol Used for \\(\\implies\\) Implies \\(\\forall\\) For All \\(\\exists\\) There exists \\(\\Leftrightarrow\\) Equivalence"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#sets","title":"Sets","text":""},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#some-basic-sets-to-remember","title":"Some basic sets to remember","text":"<ul> <li>$\\R \\to $ Set of Real Numbers</li> <li>$\\R_+ \\to $ Set of Positive Real numbers (including \\(0\\))</li> <li>\\(\\Z \\to\\) Set of all integers</li> <li>\\(\\Z_+ \\to\\) Set of positive integers including \\(0\\)</li> <li>\\([a,b] \\to \\{x\\in\\R\\ |\\ a\\leq x\\leq b\\}\\)</li> <li>\\((a,b) \\to \\{x\\in\\R\\ |\\ a&lt; x&lt; b\\}\\)</li> <li>$\\R^d \\to $  Set of \\(d\\)-dimensional vectors</li> <li>\\([a,b]^d \\to \\{x\\in\\R^d:x_i\\in[a,b], i\\in\\{1,2,3,\\dots d\\}\\}\\)</li> </ul>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#metric-spaces","title":"Metric Spaces","text":"<p>Metric spaces are basically vectors with some Distance function \\(D\\) associated with them.</p> <p>For most of question throughout the course the default metric space used is \\(\\R^d\\) where \\(D=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2+\\dots(x_d-y_d)^2}\\),</p>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#open-ball-in-metric-space","title":"Open ball in Metric Space","text":"<p>Let \\(x\\in\\R^d\\). Then, the open ball is given by \\(B(x,\\epsilon)=\\{y\\in\\R^d: D(x,y)&lt;\\epsilon\\}\\)</p>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#closed-ball-in-metric-space","title":"Closed ball in Metric Space","text":"<p>Let \\(x\\in\\R^d\\). Then, the closed ball is given by \\(B(x,\\epsilon)=\\{y\\in\\R^d: D(x,y)\\leq\\epsilon\\}\\)</p> <p>Tip</p> <p>If you include the boundary, it is a closed ball, else an open ball.</p> <p>Some basic operations defined on sets are</p> <ul> <li>Union (\\(\\cup\\))</li> <li>Intersection (\\(\\cap\\))</li> <li>Complement (\\(S^{c}\\))</li> <li>\\(A-B\\)</li> </ul>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#union-cup","title":"Union (\\(\\cup\\))","text":"<p>The union of two sets \\(A\\) and \\(B\\) is the set of elements which are in \\(A\\), as well as in \\(B\\) or in both.</p>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#intersection-cap","title":"Intersection (\\(\\cap\\))","text":"<p>The intersection of two sets \\(A\\) and \\(B\\) is the set of elements which are in both \\(A\\) and \\(B\\).</p>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#complement-sc","title":"Complement (\\(S^{c}\\))","text":"<p>The complement of a set \\(S\\) is the set of all elements which are in the universal set but not in \\(S\\).</p>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#a-b","title":"\\(A-B\\)","text":"<p>This contains all the elements of \\(A\\) that are not in \\(B\\).</p>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#de-morgans-law","title":"De Morgan's Law","text":"<p>Let there be two sets \\(A\\) and \\(B\\). Then, the De-Morgan's Law states that,</p> <ul> <li>\\((A\\cup B)^{c}=A^c\\cap B^c\\)</li> <li>\\((A\\cap B)^{c}=A^c\\cup B^c\\)</li> </ul>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#sequences","title":"Sequences","text":"<p>An ordered collection of elements is known as a sequence.</p> <p>Let there be a sequence where the elements are \\(x_1,x_2,...\\), where \\(x_i\\in\\R^d\\). And, if \\(lim_{i\\to\\infin} x^i = x^*\\) then, \\(\\forall\\text{  } \\epsilon&gt;0, \\exists \\text{ a }N, \\text{ such that } x_n\\in B(x^*, \\epsilon)\\ \\forall\\ n\\geq N\\)</p>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#vector-spaces","title":"Vector Spaces","text":"<p>A vector space is a set of elements called vectors, having operations of addition and scalar multiplication defined on it.</p> <p>A vector space must satisfy the following properties:</p> <ul> <li>Additive Closure: If the vector space is \\(V\\) and \\(v_1,v_2\\in V\\), then \\(c_1v_1 + c_2v_2\\in V\\), where \\(c_1,c_2\\in \\R\\)</li> </ul> <ul> <li>Multiplative Closure</li> </ul>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#dot-product-of-two-vectors","title":"Dot Product of two vectors","text":"<p>Let \\(x\\) and \\(y\\) be two vectors such that \\(x,y\\in\\R^d\\). Then, \\(x.y=x^Ty=\\sum_{i=1}^dx_i.y_i\\)</p>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#norm-of-a-vector","title":"Norm of a vector","text":"<p>Let \\(x\\) be a vector such that \\(x\\in\\R^d\\). Then, the norm of the vector is given by \\(||x||=\\sqrt{x^Tx}=\\sqrt{\\sum_{i=1}^dx_i^2}\\)</p>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#orthognal-vectors","title":"Orthognal Vectors","text":"<p>Let \\(x\\) and \\(y\\) be two vectors such that \\(x,y\\in\\R^d\\). Then, \\(x\\) and \\(y\\) are orthogonal if \\(x.y=0\\), i.e., \\(x^Ty=0\\)</p>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#functions-and-graphs","title":"Functions And Graphs","text":"<p>Let \\(f\\) be a function with domain \\(A\\) and co-domain \\(B\\). Then, the function is denoted as \\(f:A\\to B\\).</p>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#d-dimensional-functions","title":"\\(d\\)-dimensional functions","text":"<p>A function with domain \\(\\R^d\\) to \\(\\R\\) is known as a \\(d\\)-dimensional function or multivariate function.</p> <p>It is denoted as \\(f:\\R^d\\to\\R\\)</p>"},{"location":"Basic%20Math%20Tools/01%20Sets%20And%20Functions/#graph-of-a-function-f","title":"Graph of a function \\(f\\)","text":"<p>The graph of a \\(d\\)-dimensional function is defined as \\(G_f \\subseteq \\R^{d+1}\\).</p> \\[ G_f=\\{(x, f(x)): x\\in\\R^d\\} \\]"},{"location":"Basic%20Math%20Tools/02%20Univariate%20Calculus/","title":"Univariate Calculus","text":"<p>A univariate function is defined as \\(f:\\R\\to\\R\\)</p>"},{"location":"Basic%20Math%20Tools/02%20Univariate%20Calculus/#continuity-of-a-univariate-function-f","title":"Continuity of a univariate function \\(f\\)","text":"<p>A function \\(f:\\R\\to\\R\\) is continous if the \\(lim_x\\to x^* f(x)\\) exists and is equal to \\(f(x^*)\\).</p> <p>Tip</p> <p>If there are no sudden breaks in the graph or the graph does not break in the domain specified than the function is continous in the specified domain.</p>"},{"location":"Basic%20Math%20Tools/02%20Univariate%20Calculus/#differentiability-of-a-univariate-function-f","title":"Differentiability of a univariate function \\(f\\)","text":"<p>A function \\(f:\\R\\to\\R\\) is differentiable if the \\(f'(x^*) = lim_{h\\to 0} \\frac{f(x^*+h)-f(x^*)}{h}\\) exists.</p> <p>Warning</p> <p>If \\(f\\) is not continous at \\(x^*\\), then the function is not differentiable at \\(x^*\\). But if a function is not differentiable at \\(x^*\\),then it is not necessary that the function is not continous at \\(x^*\\).</p> <p>Tip</p> <p>If the graph of the function has no sudden breaks or no sharp corners in the specified domain, then the function is differentiable in the specified domain.</p>"},{"location":"Basic%20Math%20Tools/02%20Univariate%20Calculus/#derivatives-and-linear-approximation-of-a-function-f","title":"Derivatives and Linear Approximation of a function \\(f\\)","text":"<p>Let \\(f:\\R\\to\\R\\) be a differentiable function.</p> <p>Then the derivate of the function \\(f\\) at \\(x^*\\) is defined as</p> \\[ f'(x^*) = lim_{x\\to x^*}\\cfrac{f(x)-f(x^*)}{x-x^*}\\\\ \\ \\\\ \\text{Let } f'(x^*) \\approx \\cfrac{f(x)-f(x^*)}{x-x^*}\\text{  (Around } x=x*)\\\\\\ \\\\ f(x) \\approx f(x^*)+f'(x*)(x-x^*) \\] <p>Now \\(f(x^*)+f'(x*)(x-x^*)\\) is known as linear approximation \\(L_{x^*}[f](x)\\) of \\(x\\) around \\(x=x^*\\).</p> <p>Tip</p> <p>$f(x)\\approx L_{x^}f $, where $ L_{x^}f =f(x^)+f'(x)(x-x^*)$</p>"},{"location":"Basic%20Math%20Tools/02%20Univariate%20Calculus/#linear-approximations-and-tangent-lines","title":"Linear Approximations And Tangent Lines","text":"<p>Graph of $ L_{x^*}[f]$, i.e. \\(G_{ L_{x^*}[f]}\\) is a tangent to the graph of \\(f\\), i.e., \\(G_f\\) at the point \\((x^*,f(x^*))\\)</p>"},{"location":"Basic%20Math%20Tools/02%20Univariate%20Calculus/#some-rules-and-advanced-applications","title":"Some rules and Advanced Applications","text":""},{"location":"Basic%20Math%20Tools/02%20Univariate%20Calculus/#higher-order-approximations","title":"Higher order Approximations","text":""},{"location":"Basic%20Math%20Tools/02%20Univariate%20Calculus/#product-rule","title":"Product Rule","text":"<p>Let \\(f(x) = g(x).h(x)\\) and \\(x^*=0\\). We need to find \\(f'(x)\\).</p> <p>Now, using linear approximation and approximating \\(g(x)\\) and \\(f(x)\\) seprately.</p> \\[ f(x) \\approx (g(0) + xg'(0))(h(0) + xh'(0))\\\\ \\ \\\\ f(x) \\approx g(0)h(0) + x[g'(0)h(0)+h'(0)g(0)]+x^2g'(0)h'(0) \\dots (1)\\\\ \\ \\\\ \\ \\\\ \\text{ We know that }, L_{x^{*}}[f](0) = f(0) + xf'(0)\\dots(2) \\] <p>Comparing \\((1)\\) with \\((2)\\) we get \\(f'(0) = g'(0)h(0) + g(0)h'(0)\\)</p>"},{"location":"Basic%20Math%20Tools/02%20Univariate%20Calculus/#chain-rule","title":"Chain rule","text":"<p>Let \\(f(x) = g(h(x))\\).</p> <p>Now approximating \\(h(x)\\) around \\(0\\). We get, \\(f(x) \\approx g(h(0) + xh'(0))\\)</p> <p>Now approximating \\(g(x)\\) around \\(h(0)\\). We get, \\(f(x) \\approx g(h(0)) + g'(h(0))h'(0)x \\dots (1)\\)</p> <p>We know that \\(L_{x^*}[f](0) = g(h(0))+f'(0)x \\dots(2)\\)</p> <p>Comparing \\((1)\\) and \\((2)\\), We get, \\(f'(0) = g'(h(0)).h'(0)\\)</p>"},{"location":"Basic%20Math%20Tools/02%20Univariate%20Calculus/#maxima-minima-and-saddle-point","title":"Maxima, Minima And Saddle Point","text":"<p>Since \\(L_{x^*}[f] = f(x^*)+f'(x^*)(x-x^*)\\) and if \\(f'(x^*) = 0\\), then the linear approximation of \\(f(x)\\) becomes a constant and this happens only at either the maxima, minima or a saddle point.</p> <p>Hence if \\(f'(x^*) = 0\\), then \\(x^*\\) is a critical point.</p>"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/","title":"Multivariate Calculus","text":"<p>Let a function \\(f\\) be defined as \\(f:\\R^d\\to\\R\\). This will be used to reference each multivariate function described in this article.</p>"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/#lines","title":"Lines","text":"<ul> <li>A line in \\(\\R^d\\) is a subset of \\(\\R^d\\)</li> <li>A line through point \\(u\\in\\R^d\\) along the vector \\(x\\in\\R^d\\) is given by \\(\\{x\\in\\R^d: x= u +\\alpha v, \\alpha\\in\\R\\}\\)</li> <li>A line through two points \\(u,u'\\in\\R^d\\) is given by \\(\\{x\\in\\R^d:x=u+\\alpha(u-u'), \\alpha\\in\\R\\}\\)</li> </ul>"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/#hyper-plane","title":"Hyper Plane","text":"<ul> <li>A hyperplane of \\(d-1\\) dimensions \\(\\subseteq R^d\\).</li> <li>A hyperplane perpendicular to a vector \\(w\\in\\R^d\\) with a value \\(b\\in\\R\\) is given by \\(\\{x\\in\\R^d:w^Tx=b\\} = \\{x\\in\\R^d:\\sum_{i=1}^dw_ix_i=b\\}\\) </li> </ul>"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/#partial-derivatives","title":"Partial Derivatives","text":"<p>The partial derivative of \\(f\\) is defined as the derivative of \\(f\\) with respect to one of the variables, keeping the other variables constant, i.e.,</p> <p>$$ \\cfrac{\\partial f}{\\partial x_i}(v) = lim_{\\alpha\\to{0}}\\cfrac{f(v+\\alpha e_i)-f(v)}{\\alpha} $$ Here \\(e_i\\) is the \\(i^{th}\\) unit vector in \\(\\R^d\\)</p> <p>Let \\(f:\\R^d\\to\\R\\) and \\(f=1x_1+2x_2+\\dots nx_n\\)</p> <p>Then the partial derivative of \\(f\\)  - with respect to the variable \\(x_1\\) is  $$ \\cfrac{\\partial f}{\\partial x_1} = 1 $$ - with respect to the variable \\(x_2\\) is  $$ \\cfrac{\\partial f}{\\partial x_2} = 2 \\\\ \\\\vdots $$ and so on</p>"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/#gradients","title":"Gradients","text":"<p>Let \\(f:\\R^d\\to\\R\\) be defined.</p> <p>Then \\(\\cfrac{\\partial f}{\\partial x} = \\begin{bmatrix}\\cfrac{\\partial f}{\\partial x_1},\\cfrac{\\partial f}{\\partial x_2}, \\cfrac{\\partial f}{\\partial x_3}, \\cfrac{\\partial f}{\\partial x_4}, \\cfrac{\\partial f}{\\partial x_5}, \\dots, \\cfrac{\\partial f}{\\partial x_d}, \\end{bmatrix}\\)</p> <p>Hence the gradient is equal to \\([\\cfrac{\\partial f}{\\partial x}]^T\\) , i.e., $$ \\cfrac{\\triangledown f}{\\triangledown x} = \\begin{bmatrix} \\cfrac{\\partial f}{\\partial x_1}\\\\ \\ \\cfrac{\\partial f}{\\partial x_2}\\ \\ \\ \\vdots\\\\ \\\\ \\cfrac{\\partial f}{\\partial x_n} \\end{bmatrix} $$</p>"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/#linear-approximations-and-gradients","title":"Linear Approximations and Gradients","text":"<p>Let \\(f\\) be a function defined from \\(\\R^d\\to\\R\\)</p> <p>Then the linear approximation of \\(f\\) at a vector \\(v\\in\\R^d\\) is given by, </p> \\[ L_v[f](x)= f(x) = f(v) + \\triangledown f(v)^T(x-v) \\]"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/#gradients-and-tangent-planes","title":"Gradients and Tangent Planes","text":"<p>The graph of \\(L_v[f]\\) is a plane that is tangent to the graph of \\(f\\) at the point \\((v, f(v))\\)</p>"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/#gradients-and-contours","title":"Gradients and Contours","text":"<p>The gradient of \\(f\\) evaluated at \\(v\\) is \\(\\perp\\) to the level set of \\(f.\\) Mathematically,</p> \\[ \\triangledown f(v) \\perp \\{x\\in\\R^d:f(x)=f(v)\\} \\]"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/#proof","title":"Proof","text":"<p>$$</p> <p>{x\\in\\R^d:f(x)=f(v)}\\newline \\ \\newline \\rightarrow{x\\in\\R^d:L_vf=f(v)}\\newline \\ \\newline \\rightarrow{x\\in\\R^d:f(v)+\\triangledown f(v)^T(x-v)=f(v)}\\newline \\ \\newline</p> <p>\\rightarrow  {x\\in\\R^d:\\triangledown f(v)^Tx=\\triangledown f(v)^Tv}\\newline \\ \\newline \\text{Comparing with } {x\\in\\R^d:W^Tx=b}, \\text{where W is }\\perp\\text{to the plane}\\newline\\ \\newline \\therefore \\triangledown f(v) \\perp {x\\in\\R^d:L_vf=f(v)} $$</p>"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/#directional-derivative","title":"Directional Derivative","text":"<p>The directional derivative of the function \\(f\\) at the point \\(v\\) along the direction of \\(u\\) is given by,</p> \\[ D_u[f](v) = lim_{\\alpha\\to 0} \\cfrac{f(v+\\alpha u)-f(v)}{\\alpha}\\newline\\ \\newline=lim_{\\alpha\\to 0} \\cfrac{L_v[f](v)-f(v)}{\\alpha}\\newline\\ \\newline=lim_{\\alpha\\to 0} \\cfrac{f(v)+\\triangledown f(v)^T(\\alpha u)-f(v)}{\\alpha}\\newline\\ \\newline=\\triangledown f(v)^Tu \\]"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/#cauchy-schwarz-inequality","title":"Cauchy-Schwarz Inequality","text":"<p>Let \\(a,b\\in\\R^d\\). Then the equality states that</p> \\[ -||a||.||b|| \\leq a^Tb \\leq ||a||.||b|| \\]"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/#points-to-note","title":"Points to Note","text":"<ol> <li>\\(-||a||.||b|| = a^Tb\\), if and only if, \\(a=\\alpha b\\) and \\(a&lt;0\\)</li> <li>\\(||a||.||b|| = a^Tb\\), if and only if, \\(a=\\alpha b\\) and \\(a&gt;0\\)</li> </ol>"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/#direction-of-steepest-ascent","title":"Direction of steepest ascent","text":"<p>For a function \\(f\\), the direction of a unit vector \\(u\\) such that it maximises the directional derivative of \\(f\\) along \\(u\\) is \\(u=\\triangledown f(v)\\), i.e., for steepest ascent \\(u\\) should be in direction of \\(\\triangledown f\\)</p>"},{"location":"Basic%20Math%20Tools/03%20Multivariate%20Calculus/#direction-of-steepest-descent","title":"Direction of steepest descent","text":"<p>For a function \\(f\\), the direction of a unit vector \\(u\\) such that it minimises the directional derivative of \\(f\\) along \\(u\\) is \\(u=-\\triangledown f(v)\\), i.e., for steepest descent \\(u\\) should be in direction opposite to \\(\\triangledown f\\)</p>"},{"location":"Basic%20Math%20Tools/04%20Complex%20Numbers/","title":"Complex Numbers","text":"<p>A number containing the term \\(i\\), i.e., \\(\\sqrt{-1}\\) is known as a complex number.</p> <p>It is of the form \\(a+ib,\\) where \\(a,b\\in \\R\\) and \\(a\\) is known as the real part and \\(ib\\) is known as the imaginary part.</p>"},{"location":"Basic%20Math%20Tools/04%20Complex%20Numbers/#addition-of-complex-numbers","title":"Addition of Complex Numbers","text":"<p>Adding complex numbers gives a new complex number, whose real part is the sum of the real parts of the orignal complex numbers and the imaginary part is the sum of the imaginary parts of the orignal complex numbers.</p> <p>\\((a+ib) + (c+id) = (a+b) + i(b+d)\\)</p>"},{"location":"Basic%20Math%20Tools/04%20Complex%20Numbers/#multiplication-in-complex-numbers","title":"Multiplication in complex numbers","text":"<p>\\((a+ib)(c+id)=a.c+i(a.d)+i(b.c)-b.d\\newline\\to(a+ib)(c+id)=(ac-bd)+i(ad+bc)\\)</p>"},{"location":"Basic%20Math%20Tools/04%20Complex%20Numbers/#complex-conjugate","title":"Complex Conjugate","text":"<p>In complex number, a term called complex conjugate is defined.</p> <p>The complex conjugate of a complex number \\(a+ib\\) is valued as \\(a-ib.\\)</p>"},{"location":"Basic%20Math%20Tools/04%20Complex%20Numbers/#length-of-aib","title":"Length of \\(a+ib\\)","text":"<p>Length or distance of \\(a+ib\\) from origin \\(=\\sqrt{a^2+b^2}\\)</p>"},{"location":"Linear%20Algebra/01%20Four%20fundamental%20Subspaces/","title":"Four Fundamental Subspaces","text":"<p>In this article, we discuss about the four fundamental subspaces related to a matrix. They are - Column space - Row Space - Null Space - </p>"},{"location":"Linear%20Algebra/01%20Four%20fundamental%20Subspaces/#column-space-ca","title":"Column Space \\(C(A)\\)","text":"<p>Let a matrix \\(A\\) consists of columns \\(u_1,u_2,\\dots u_n\\). Then \\(C(A) = \\text{span}(u_1,u_2,u_3\\dots u_n)\\)</p>"},{"location":"Linear%20Algebra/01%20Four%20fundamental%20Subspaces/#solving-axb-using-ca","title":"Solving \\(Ax=b\\) using \\(C(A)\\)","text":"<p>\\(A(x)=b\\) is solvable if and only if \\(b\\in C(A)\\)</p> <p>Let us look at an example to see how we use the \\(C(A)\\) to solve \\(Ax=b\\)</p> \\[ \\text{Let }A = \\begin{bmatrix} 1\\ 1\\ 2\\newline2\\ 1\\ 3\\newline3\\ 1\\ 4\\newline 4\\ 1\\ 5 \\end{bmatrix} \\] <p>Is \\(Ax=b\\) solvable? No, because \\(C(A)=\\text{span}(C_1, C_2)\\) and \\(C_3 = C_1+C_2\\)  whereas the variables are only \\(3\\). Hence there are \\(4\\) equations and \\(3\\) unknowns and \\(C(A)\\) is a 2-dimensional subspace of \\(\\R^4.\\)</p>"},{"location":"Linear%20Algebra/01%20Four%20fundamental%20Subspaces/#row-space","title":"Row space","text":"<p>The row space of a matrix \\(A\\) is defined as column space of \\(A^T\\), i.e., \\(R(A)=C(A^T)\\)</p> <p>Note</p> <p>Column Rank \\(=\\) Row Rank</p>"},{"location":"Linear%20Algebra/01%20Four%20fundamental%20Subspaces/#null-space","title":"Null Space","text":"<p>Null space of a matrix \\(A\\) is defined as \\(N(A)=\\{x|Ax=0\\}\\)</p> <p>Remember</p> <p>If \\(A\\) is invertible matrix, then \\(N(A) = 0\\) and \\(C(A)\\) is the entire space. It so happens that \\(Ax=b\\) has a unique solution \\(x=A^{-1}b\\)</p> <p>Tip</p> <p>We can also use the Gaussian elimination to find the null space of a matrix \\(A.\\) </p>"},{"location":"Linear%20Algebra/01%20Four%20fundamental%20Subspaces/#rank-nullity-theorem","title":"Rank Nullity Theorem","text":"<p>In the gaussian elimination, the number of pivot columns is called the Rank of the matrix, i.e., Rank \\(=\\) Dim \\((C(A)).\\)</p> <p>And the number of free variables is known as Nulity of the matrix, i.e., Nullity \\(=\\) Dim \\((N(A)).\\)</p> <p>Now according to the theorem, if \\(A\\) has \\(n\\) columns, then $$ n=\\text{Rank} + \\text{Nullity} $$</p>"},{"location":"Linear%20Algebra/01%20Four%20fundamental%20Subspaces/#left-null-space","title":"Left Null Space","text":"<p>The Left Null Space of a matrix \\(A\\) is defined as the null space of \\(A^T.\\)</p> \\[ \\text{Left Null Space}=\\{y|A^Ty=0\\}=\\{y|y^TA=0\\} \\] <p>Tip</p> <p>Row Space \\(+\\) Left Null Space \\(=\\) Row Space</p>"},{"location":"Linear%20Algebra/02%20Orthagonal%20Vectors%20and%20Subspaces/","title":"Orthogonal Vectors and Subspaces","text":""},{"location":"Linear%20Algebra/02%20Orthagonal%20Vectors%20and%20Subspaces/#length-of-a-vector","title":"Length of a vector","text":"<p>Let a vector \\(x\\) be defined such that \\(x\\in\\R^n\\) $$ \\text{Length of vector }x=||x||=\\sqrt{x_1^2+x_2^2+...x_n^2} $$</p>"},{"location":"Linear%20Algebra/02%20Orthagonal%20Vectors%20and%20Subspaces/#orthogonality","title":"Orthogonality","text":"<p>Two vectors \\(x\\) and \\(y\\) are said to be orthogonal \\((\\perp)\\), if \\(x^Ty=y^Tx=0\\)</p> <p>Tip</p> <ul> <li>\\(0\\) vector is orthogonal to every vector \\(x\\).</li> <li>If there are \\(k\\) mutually orthogonal vectors, then the set of that vectors are linearly independent.</li> </ul>"},{"location":"Linear%20Algebra/02%20Orthagonal%20Vectors%20and%20Subspaces/#orthonormal-vectors","title":"Orthonormal Vectors","text":"<p>Let two vectors \\(u,v\\in\\R^n\\).</p> <p>\\(u,v\\) are said to be orthonormal, if they are both orthogonal and of unit length. Mathematically,</p> \\[ u^Tv=v^Tu=0 \\text{ and } ||u||=||v||=1 \\]"},{"location":"Linear%20Algebra/02%20Orthagonal%20Vectors%20and%20Subspaces/#orthogonal-subspaces","title":"Orthogonal Subspaces","text":"<p>Let \\(U, V\\) be vector subspaces.</p> <p>\\(U,V\\) are vector subspaces if \\(u^Tv=v^Tu=0\\), where, \\(v\\in V,u\\in U\\)</p>"},{"location":"Linear%20Algebra/02%20Orthagonal%20Vectors%20and%20Subspaces/#orthogonal-subspaces-and-fundamental-subspaces","title":"Orthogonal Subspaces and Fundamental Subspaces","text":""},{"location":"Linear%20Algebra/02%20Orthagonal%20Vectors%20and%20Subspaces/#raperp-na","title":"\\(R(A)\\perp N(A)\\)","text":"<p>Let \\(x\\in N(A)\\), i.e., \\(Ax=0\\) which is equal to $$ \\begin{bmatrix} \\text{row 1}\\ \\text{row 2}\\ \\text{row 3}\\ \\end{bmatrix} \\begin{bmatrix} x_1\\ x_2\\ x_3 \\end{bmatrix}= \\begin{bmatrix} 0 \\ 0\\ 0 \\end{bmatrix} $$</p> <p>The above equation implies \\(\\text{(row 1)}\\perp x_1, \\text{(row 2)}\\perp x_2, ...\\) and so on.</p> <p>Thus for any \\(x\\), \\(\\alpha_1r_1+\\alpha_2r_2+...\\alpha_nr_n\\perp x,\\) where \\(r_i\\) is a \\(i^{th}\\) row of the matrix.</p> <p>So \\(R(A)\\perp N(A)\\implies C(A^T)\\perp N(A)\\)</p>"},{"location":"Linear%20Algebra/02%20Orthagonal%20Vectors%20and%20Subspaces/#caperp-nat","title":"\\(C(A)\\perp N(A^T)\\)","text":"<p>Try it yourself.</p> <p>Tip</p> <p>Follow the previous proof with the Column Space and Left Null space.</p>"},{"location":"Linear%20Algebra/03%20Projections/","title":"Projections of Vectors","text":""},{"location":"Linear%20Algebra/03%20Projections/#motivation","title":"Motivation","text":"<p>You might have this question that What is the need of projection?</p> <p>Let us say we are given some data \\((x_1,b_1),(x_2,b_2),...(x_n,b_n)\\) and it is given that this does not have a solution, i.e., \\(b\\notin C(A).\\) At this point, it makes sense to get a projection of \\(b\\) onto \\(A\\).</p>"},{"location":"Linear%20Algebra/03%20Projections/#projecting-a-vector-b-onto-line-a","title":"Projecting a vector \\(b\\) onto line \\(a\\)","text":"\\[ \\text{We know, }p = \\hat{x}a\\newline \\text{ and } E=b-a\\rightarrow E=b-\\hat{x}a\\\\\\text{ and } E\\perp a \\rightarrow b-\\hat{x}a\\perp a \\\\ \\rightarrow a(b-\\hat{x}a) = 0\\\\ \\rightarrow \\hat{x} = \\cfrac{a^Tb}{a^Ta}\\\\ \\ \\\\ \\text{Since, }p=\\hat{x}a\\\\\\text{Hence, } p = \\cfrac{a^Tb}{a^Ta}a \\]"},{"location":"Linear%20Algebra/03%20Projections/#projection-matrix-p","title":"Projection Matrix \\(P\\)","text":"<p>We know \\(p = \\cfrac{a^Tb}{a^Ta}a = \\cfrac{aa^T}{a^Ta}b\\)</p> <p>Then the Projection Matrix \\(P=\\cfrac{aa^T}{a^Ta}\\) and the projection of \\(b\\) onto \\(a\\) is \\(Pa.\\)</p>"},{"location":"Linear%20Algebra/03%20Projections/#important-observations","title":"Important Observations","text":"<ol> <li>\\(P\\) is always symmetric.</li> <li>\\(P^2=P, \\text{ i.e, }P^2b=Pb\\)</li> <li>Column space of \\(P =\\) line through \\(a\\) </li> <li>Null space of \\(P=\\) plane \\(\\perp a\\)</li> <li>Rank \\(P=1\\)</li> </ol>"},{"location":"Linear%20Algebra/03%20Projections/#projection-onto-a-subspace","title":"Projection onto a subspace","text":"<p>Let us have a system of equations, i.e., \\(Ax=b\\) and \\(A\\) is a \\(m\\text{ x }n\\) matrix, where \\(m&gt;n.\\)</p> <p>Let us now see how to project \\(b\\) onto column space \\(C(A)\\)</p> <p></p> <p>Now projection of \\(b\\) onto \\(S\\) is \\(p=A\\hat{x}.\\)</p> <p>We also know that \\(E=b-p=b-A\\hat{x}.\\)</p> <p>We can observe that \\(E\\perp\\text{every vector in }C(A)\\) and we also know that \\(C(A)\\perp N(A^T)\\) from here</p> <p>So we can say that \\(E\\in N(A^T)\\newline\\to A^TE=0\\newline\\to A^T(b-A\\hat{x}) = 0\\newline\\to A^TA\\hat{x}=A^Tb\\)</p> <p>Tip</p> <p>Even if \\(Ax=b\\) does not have a solution \\(A^TA\\hat{x}=A^Tb\\) has a solution</p>"},{"location":"Linear%20Algebra/03%20Projections/#important-observations_1","title":"Important Observations","text":"<ol> <li> <p>If columns of \\(A\\) are linearly independent, then \\(A^TA\\) is invertible as well as symmetric. Therefore \\(A^TA\\hat{x}=A^Tb\\) becomes \\(\\hat{x}=(A^TA)^{-1}A^Tb.\\) So projection becomes \\(A\\hat{x}=A(A^TA)^{-1}A^Tb\\)</p> </li> <li> <p>If \\(b\\in C(A),\\) i.e. \\(b=Ax\\) then the projection is the vector \\(b\\) itself.</p> </li> <li> <p>If \\(b\\in N(A^T),\\) i.e. \\(A^Tb=0\\) then the projection \\(p=0.\\)</p> </li> <li> <p>If \\(C(A)=\\R^n\\) then projection \\(p=b\\)</p> </li> <li> <p>The projection matrix is always symmetric and satisfies \\(p^2=p\\)</p> </li> </ol>"},{"location":"Linear%20Algebra/04%20Least%20Squares%20and%20Projections/","title":"Least Squares and Projections","text":"<p>Let us say that there is a system of equations \\(Ax=b\\) and the vector \\(b\\) leads to an inconsistent system.</p> <p>Let the data be of the form \\((x_1,b_1),(x_2,b_2)...(x_n,b_n)\\)</p> <p>Now we want to solve this system, so what we could do is find out the errors and try to minimise them.</p> <p>Thus my squared error is \\(E^2=(b_1-x_1)^2+(b_2-x_2)^2...+(b_n-x_n)^2\\) If we try to minimise it using calculus, i.e., \\(\\cfrac{d E^2}{d x}=0\\)</p> \\[ \\cfrac{d E^2}{d x}=0 \\\\\\ \\\\\\rightarrow 2[2(b_1-\\cfrac{dx_1}{dx})+2(b_2-\\cfrac{dx_2}{dx})+...+2(b_n-\\cfrac{dx_n}{dx})] = 0 \\\\\\ \\\\\\rightarrow \\text{On solving, this looks similar to, }\\cfrac{a^Tb}{a^Ta} \\] <p>Note</p> <p>Taking derivative and finding the minima actually turns out to be the same as peforming a projection on a subspace. Solving \\(A^TA\\hat{x}=A^Tb\\) leads to an \\(\\hat{x}\\) that minimises \\(||Ax-b||^2\\)</p>"},{"location":"Linear%20Algebra/05%20Linear%20regression/","title":"Linear Regression","text":""},{"location":"Linear%20Algebra/05%20Linear%20regression/#given-data","title":"Given data","text":"<p>Let us consider a set of datapoints \\(\\{(x_1,y_1),(x_2,y_2),\\dots(x_n,y_n)\\},\\) where \\(x_i\\in\\R^d.\\)</p> <p>Let us define a loss function \\(L(\\theta)=\\sum_{i=1}^{n}(x^T_i\\theta-y_i)^2\\)</p>"},{"location":"Linear%20Algebra/05%20Linear%20regression/#basic-setup","title":"Basic Setup","text":"<p>Let us define a feature matrix \\(A=\\begin{bmatrix}x_1^T\\\\x_2^T\\\\.\\\\.\\\\.\\\\x^T_n\\end{bmatrix}\\) </p> <p>and another matrix \\(Y=\\begin{bmatrix}y_1\\\\y_2\\\\.\\\\.\\\\.\\\\y_n\\end{bmatrix}\\)</p> <p>Now, \\(A\\theta=\\begin{bmatrix}x_1^T\\theta\\\\x_2^T\\theta\\\\.\\\\.\\\\.\\\\x^T_n\\theta\\end{bmatrix}\\)</p> <p>Now \\((A\\theta-Y)=\\begin{bmatrix}x_1^T\\theta-y_1\\\\x_2^T\\theta-y_2\\\\.\\\\.\\\\.\\\\x^T_n\\theta-y_n\\end{bmatrix}\\)</p> <p>Now \\((A\\theta-Y)^T(A\\theta-Y)=\\begin{bmatrix}(x_1^T\\theta-y_1)^2\\\\(x_2^T\\theta-y_2)^2\\\\.\\\\.\\\\.\\\\(x^T_n\\theta-y_n)^2\\end{bmatrix}=\\sum_{i=1}^{n}(x^T_i\\theta-y_i)^2\\)</p> <p>So now, the loss function \\(L=(A\\theta-Y)^T(A\\theta-Y)\\)</p>"},{"location":"Linear%20Algebra/05%20Linear%20regression/#minimising-loss-function","title":"Minimising Loss function","text":"<p>To minimise the loss function \\(L,\\) we equate the gradient of loss function to \\(0,\\) i.e., \\(\\nabla_\\theta L(\\theta)=0\\)</p> <p>\\(\\to\\nabla_\\theta((A\\theta-Y)^T(A\\theta-Y)) = 0\\)</p> <p>\\(\\to A^T(A\\theta-Y)=0\\)</p> <p>\\(\\to (A^TA)\\theta=A^TY\\)</p> <p>Note</p> <p>If \\(A\\) is full rank then, \\(\\theta=(A^TA)^{-1}A^TY.\\)</p> <p>Tip</p> <p>The application of \\(\\theta\\) is in finding the Maximum Likelihood Probability.</p>"},{"location":"Linear%20Algebra/05%20Linear%20regression/#ridge-regression-regularised-version-of-linear-regrssion","title":"Ridge Regression (Regularised version of Linear Regrssion)","text":"<p>Instead of solving the loss function \\(L(\\theta)=\\sum_{i=1}^{n}(x^T_i\\theta-y_i)^2,\\) we solve the following regularised version:</p> <p>\\(\\bar{L}(\\theta)=\\sum_{i=1}^{n}(x^T_i\\theta-y_i)^2+\\lambda||\\theta||^2,\\) where \\(\\lambda||\\theta||^2\\) is the regularization term and then minimise it.</p> <p>Now, if you minimise this using the above calculations, we get \\((A^TA+\\lambda I)\\theta_{reg}=A^TY\\to\\theta=(A^TA+\\lambda I)^{-1}A^TY\\)</p> <p>Note</p> <p>$(A^TA+\\lambda I) $ is invertible even if \\(A\\) is not of full rank</p>"},{"location":"Linear%20Algebra/06%20Polynomial%20Regression/","title":"Polynomial Regression","text":"<p>It is a method of fitting a \\(n\\) degree polynomial through a given set of datapoints</p>"},{"location":"Linear%20Algebra/06%20Polynomial%20Regression/#given-data","title":"Given Data","text":"<p>Let some one-dimensional datapoints be defined as \\(\\{(x_1,y_1),(x_2,y_2),\\dots(x_n,y_n)\\}\\) where \\((x_i,y_i)\\in R\\ \\forall\\ i\\)</p>"},{"location":"Linear%20Algebra/06%20Polynomial%20Regression/#basic-setup","title":"Basic Setup","text":"<p>Let \\(\\hat{y}(x) = \\theta_1+\\theta_2x+\\theta_3x^2\\dots\\theta_nx^n = \\sum_{j=0}^n\\phi_j(x),\\) where \\(\\phi_j(x)=x^j\\)</p> <p>Therefore, for a given \\(x,\\) \\(\\phi(x)=(1,x,x^2,\\dots x^n)\\)</p> <p>Now, \\(\\hat{y}(x) = \\theta^T\\phi(x)\\)</p>"},{"location":"Linear%20Algebra/06%20Polynomial%20Regression/#applying-linear-regression","title":"Applying Linear Regression","text":"<p>Now we apply linear regression on $\\phi(x), $</p> <p>So here \\(A=\\begin{bmatrix}\\phi(x_1)^T\\\\\\phi(x_2)^T\\\\.\\\\.\\\\.\\\\\\phi(x_n)^T\\end{bmatrix}.\\)</p> <p>Now \\((A^TA)\\theta=A^TY\\)</p>"},{"location":"Linear%20Algebra/07%20EigenValues%20and%20EigenVectors/","title":"Eigen Values and Eigen Vectors","text":""},{"location":"Linear%20Algebra/07%20EigenValues%20and%20EigenVectors/#eigen-values","title":"Eigen Values","text":"<p>Let us consider a matrix \\(A,\\) a vector \\(v\\) and a scalar \\(\\lambda,\\) if the matrix \\(A\\) satisfies the equation,</p> \\[ Av=\\lambda v \\] <p>then, the values that \\(\\lambda\\) attains are known as Eigen Values.</p>"},{"location":"Linear%20Algebra/07%20EigenValues%20and%20EigenVectors/#how-to-find-eigen-values","title":"How to find Eigen Values?","text":"<p>To find eigen values, let us do some manipulation of the equation above,</p> \\[ Av=\\lambda v\\newline \\to Av=\\lambda Iv\\newline \\to Av-\\lambda Iv=0\\newline \\to v(A-\\lambda I) = 0 \\] <p>So now for non trivial solutions of the equation above \ud83d\udc46, i.e., \\(v\\neq0\\) vector, we solve \\(A-\\lambda I=0.\\)</p> <p>The values of \\(\\lambda\\) thus obtained are Eigen Values</p>"},{"location":"Linear%20Algebra/07%20EigenValues%20and%20EigenVectors/#eigen-vectors","title":"Eigen Vectors","text":"<p>Now as we have learnt about Eigen Values we will now learn about Eigen Vectors.</p> <p>Eigen vectors are basically the values of vector \\(v\\) corresponding to each eigenvalue \\(\\lambda,\\) </p>"},{"location":"Linear%20Algebra/07%20EigenValues%20and%20EigenVectors/#finding-eigen-vectors","title":"Finding Eigen Vectors","text":"<p>After finding Eigen Values, put each value of \\(\\lambda\\) in the equation \\(Av=\\lambda v\\) or \\((A-\\lambda I)v=0\\) and find the value of \\(v\\)</p> <p>The values of \\(v\\) thus obtained are the eigen vectors to your matrix \\(A\\) corresponding to the eigen values \\(\\lambda.\\)</p>"},{"location":"Linear%20Algebra/07%20EigenValues%20and%20EigenVectors/#some-important-properties","title":"Some Important Properties","text":"<ol> <li>The sum of the eigen values is equal to the trace of the matrix \\(A,\\) i.e., \\(\\sum\\lambda=tr(A).\\)</li> <li>The product of the eigen values is equal to the determinant of the matrix \\(A,\\) i.e., \\(\\prod\\lambda=det(A).\\)</li> <li>A quick and easy way to find out the eigen values is by using the first two properties.</li> <li>Symmetric matrices have real eigenvalues.</li> <li>There may be some matrices for which there may be repeated eigen values.</li> <li>Power law: For a matrix \\(A\\) if there eigenvalues are \\(\\{\\lambda_1,\\lambda_2,...,\\lambda_n\\},\\) then for \\(A^k\\) the eigenvalues will be \\(\\{\\lambda_1^k, \\lambda_2^k,...\\lambda_n^k\\}\\) </li> </ol>"},{"location":"Linear%20Algebra/07%20EigenValues%20and%20EigenVectors/#importance-of-eigen-values-and-eigen-vectors","title":"Importance of Eigen Values and Eigen Vectors","text":"<p>If \\(x\\) is an eigen vector, then \\(A\\) either extends it or shrinks it by the scale of \\(\\lambda\\)</p> <p>This has been gracefully demonstrated by the professor in the lecture and if you prefer an animated version of the same, here is a great video by 3blue1brown</p>"},{"location":"Linear%20Algebra/08%20Diagonalisation%20of%20a%20matrix/","title":"Diagnolization of a matrix","text":"<p>Let us consider a matrix \\(A,\\) and there exists an invertible matrix \\(S,\\) such that \\(S^{-1}AS=B,\\) where \\(B\\) is a diagonal matrix. Then \\(A\\) is said to be diagonalizable.</p>"},{"location":"Linear%20Algebra/08%20Diagonalisation%20of%20a%20matrix/#relation-between-diagonalization-and-eigenvectors","title":"Relation between Diagonalization and EigenVectors","text":"<ul> <li>Claim: If eigenvalues are distinct then eigenvectors are linearly independent</li> </ul>"},{"location":"Linear%20Algebra/08%20Diagonalisation%20of%20a%20matrix/#observations","title":"Observations","text":"<ol> <li>For a given matrix \\(A,\\) \\(S\\) is not unique. This is because the eigenvectors can be scaled by any factor.</li> <li>Using the power property from eigenvectors and eigenvalues, we can get \\(S^{-1}A^kS=B^k\\)</li> <li>Not all matrices are diagonalisable.</li> </ol>"},{"location":"Linear%20Algebra/08%20Diagonalisation%20of%20a%20matrix/#a-quick-tip","title":"A quick tip","text":"<p>If you ponder upon the process, you can see that you \\(S\\) is a set of all the eigen vectors, i.e., \\(S=\\begin{bmatrix}x_1\\ x_2\\ x_3\\ ...\\ x_n\\end{bmatrix}\\) </p> <p>And \\(B\\) is just a diagonal matrix with all its diagonal elements as eigen values, i.e., \\(B=\\lambda I,\\) where \\(\\lambda=[\\lambda_1\\ \\lambda_2\\dots\\ \\lambda_n],\\) where \\(\\lambda_1\\) is the eigen value corresponding to eigen vector \\(x_1\\) and so on.</p>"},{"location":"Linear%20Algebra/09%20Orthogonally%20Diagnolisable%20matrix/","title":"Orthogonally Diagnolizable matrix","text":"<p>Let us say matrix \\(A\\) is diagnolizable. We say that A is orthogonally diagnolisable if and only if \\(S\\) is orthogonal, i.e., \\(SS^T=I\\) or \\(S^{-1}=S^T,\\) i.e.,</p> \\[ S^{-1}AS=B \\text{ becomes } S^TAS=B \\]"},{"location":"Linear%20Algebra/09%20Orthogonally%20Diagnolisable%20matrix/#important-points-to-note","title":"Important Points to Note","text":"<ol> <li>The matrix \\(A\\) must be real symmetric matrix to be orthagonally diagnolizable.</li> </ol>"},{"location":"Linear%20Algebra/10%20Complex%20Matrices/","title":"Complex Matrices","text":"<p>The matrices with the elements coming from the complex number system are known as Complex Matrices.</p>"},{"location":"Linear%20Algebra/10%20Complex%20Matrices/#inner-product-of-complex-vectors","title":"Inner product of Complex Vectors","text":"<p>Recall, in \\(\\R^n,\\) \\(x.y=x^Ty.\\)</p> <p>But in \\(\\mathbb{C},\\) we cannot use the same definition.</p> <p>So we define the inner product as \\(x.y=\\bar{x}^Ty\\)</p> <p>Warning</p> <p>\\(\\bar{x}^Ty\\neq\\bar{y}^Tx\\)</p>"},{"location":"Linear%20Algebra/10%20Complex%20Matrices/#length-of-a-complex-vector","title":"Length of a complex vector","text":"<p>Recall, in \\(\\R^n,\\) \\(||x||^2=x^Tx.\\)</p> <p>But in \\(\\mathbb{C}^n,\\) we cannot use the same definition.</p> <p>So we define the length as \\(||x||^2=\\bar{x}^Tx\\)</p>"},{"location":"Linear%20Algebra/10%20Complex%20Matrices/#conjugate-transpose-a","title":"Conjugate Transpose (\\(A^*\\))","text":"<p>For a complex matrix \\(A\\), the conjugate transpose \\(A^*\\) is defined as </p>"}]}